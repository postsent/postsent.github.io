---
title: NLP 
date: 2021-01-05
last_modified_at: 2021-01-05
categories:
  - Data Science
excerpt: As title
---
\#nlp

# Overview

[awesome nlp](https://github.com/keon/awesome-nlp)
  - [nlp overview](https://nlpoverview.com/)
benchmark 
  - paperswithcode - https://paperswithcode.com/sota/sentiment-analysis-on-imdb



# Pre-trained words embeddding

[fasttext](https://fasttext.cc/docs/en/english-vectors.html)
[glove]

# Pre-train model

- BERT (from Google) released with the paper BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova.
- XLNet (from Google/CMU) released with the paper XLNet: Generalized Autoregressive Pretraining for Language Understanding by Zhilin Yang*, Zihang Dai*, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le.

- RoBERTa (from Facebook), a Robustly Optimized BERT Pretraining Approach by Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du et al.

- DistilBERT (from HuggingFace), released together with the blogpost Smaller, faster, cheaper, lighter: Introducing DistilBERT, a distilled version of BERT by Victor Sanh, Lysandre Debut and Thomas Wolf.

# Courses

[nlp standford](https://www.youtube.com/playlist?list=PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ)

# Others

[github awesome nlp list](https://github.com/keon/awesome-nlp#videos-and-online-courses)

# Visualisation

[Pre-trained word embedding demo](https://ronxin.github.io/wevi/)

# Pre-processing

[nltk tokenize](https://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize.casual)

- emoticons
- urls

techniques
- [porter stemming](https://www.nltk.org/api/nltk.stem.html#module-nltk.stem.porter)

We can see that the stop words list above contains some words that could be important in some contexts. These could be words like **i, not, between, because, won, against**. You might need to customize the stop words list for some applications. 

For the punctuation, we saw earlier that **certain groupings** like **':)'** and **'...'** should be retained when dealing with tweets because they are used to express emotions. In other contexts, like medical analysis, these should also be removed.

# Topic modelling 

- [Gensim](https://radimrehurek.com/gensim/models/word2vec.html)
- [fasttext, Facebook](https://fasttext.cc/))
  
# My template

- print str with different colors

# TODO

- [ ] english grammar: noun, pronoun, etc.